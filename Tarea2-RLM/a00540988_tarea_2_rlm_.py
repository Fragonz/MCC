# -*- coding: utf-8 -*-
"""A00540988 Tarea 2-RLM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19BtrAnNxFxdonYgOqGt50zL59ieTVbcl

# **ITESM Campus Guadalajara**
## Aprendizaje de Máquinas
## Tarea 2 Regresión Lineal Múltiple

### **A00540988 Luis Francisco González Rodíguez**


---

# 1.  En este ejercicio usaremos la base de datos de Boston_housing_dataset (https://www.kaggle.com/apratim87/housingdata) la cual consta de 13 variables independientes y la variable dependiente. Son 506 datos de casas cuyo objetivo es la predicción de su costo (MEDV).
"""

# Carga de librerias necesarias para leer y procesar los datos.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

#Cargar Data Set tal como se decargóó del sitio de Kaggle
!curl -O https://raw.githubusercontent.com/Fragonz/MCC/master/Tarea2-RLM/data/datasets_270_583_housingdata.csv
!ls
!pwd
dspath = "/content/datasets_270_583_housingdata.csv"

"""**Examinemos el Data Set para ver su contenido.**"""

mydata = pd.read_csv(dspath, header=None)
mydata.head()

"""---
Después de examinar el dataset podemos comprobar que este archivo no contiene **headers** y debemos agregarlos siguiendo las instrucciones del sitio de Keggle: https://www.kaggle.com/apratim87/housingdata?select=housingdata.csv el cual nos dice que las columnas estan en orden y con las siguientes etiquetas:

**Variables in order:**

* CRIM - per capita crime rate by town
* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
* CHAS - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
* NOX - nitric oxides concentration (parts per 10 million)
* RM - average number of rooms per dwelling
* AGE - proportion of owner-occupied units built prior to 1940
* DIS - weighted distances to five Boston employment centres
* RAD - index of accessibility to radial highways
* TAX - full value property tax rate per 10'000
* PTRATIO - pupil teacher ratio by town
* B 1000 - (Bk - 0.63)^2 where Bk is the proportion of blacks by town
* LSTAT % - lower status of the population
* MEDV - Median value of owner-occupied homes in $1000's

---
"""

# Agregamos las etiquetas a las columnas 
column_labels=["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]
mydata.columns = column_labels

"""**Revisamos nuestro dataset con los nombres de columnas para poder manipular nuestros datos:**

---
"""

mydata.head()

# Examinemos el tamaño del DataSet para corroborar que tenemos 506 Datos
# Además contamos con 14 columnas:
# 13 que corresponden a variables independientes y 1 variable dependiente.

mydata.shape

"""## a.  Obtener el modelo de Regresión Lineal Múltiple (RLM) del modelo y con base a dicho modelo contestar los siguientes incisos:"""

# Dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# Variables Independientes
x = dataset[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT"]]
# Variable Dependiente
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
print(xones)

"""### **Primer Modelo de Regresión Lineal Múltile (RLM)**"""

mymodel = sm.OLS(y, xones).fit()
print(mymodel.summary())

"""### i.   Interpretar el valor del coeficiente de determinación ajustado 𝑅²

El coeficiente de determinación ajustado para nuestro modelo fue de 0.734, recordemos que este mide el porcentaje de variación de la variable dependiente teniendo en cuenta las variable independientes, es decir la variación es del 73.4%

### ii. Determina qué variables no resultaron significativas considerendo un 𝑣𝑎𝑙𝑜𝑟-𝑝=0.01

Por observación en la tabla de nuestro modelo podemos concluir que las variables que no resultaron significativas, o con un *p-value* superior a 0.01 son **INDUS** con un *p-value* de 0.738 y **AGE** con un *p-value* de 0.958

### iii. Obtener un segundo modelo de RLM omitiendo las variables que no resultaron significativas en el inciso anterior. Comparar su coeficiente de determinación ajustado con base con respecto al primer modelo.

Para obtener un segundo modelo RLM, repetiremos los pasos del primer punto, pero esta vez removeremos las variables **INDUS** y **AGE**, las cuales resultaron NO significativas para nuestro modelo.
"""

# Repetimos los pasos de nuestro modelo anterior
# Definimos el DF para el dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# De nuestras variable independeintes removemos las variable INDUS y AGE que resultaron NO significativas para nuestro modelo
x = dataset[["CRIM","ZN","CHAS","NOX","RM","DIS","RAD","TAX","PTRATIO","BK","LSTAT"]]
# Definimos variable dependiente
y = dataset[["MEDV"]]

"""#### **Segundo Modelo RLM**"""

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
# Entrenamos nuestro modelo
mymodel = sm.OLS(y, xones).fit()
# Imprimimos los resultados
print(mymodel.summary())

"""#### **Comparar coeficiente de determinación ajustado con respecto al primer modelo**

* Coeficiente 𝑅² Ajustado primer modelo = 73.4%
* Coeficiente 𝑅² Ajustado segundo modelo = 73.5%

Esto significa que tuvimos una mejora de 0.001 al elininar dos variables, esto parece algo pequeño pero ahora podemos trabajar con un modelo de 11 variable y no de 13 como originalmente se había planteado.

### iv. Obtener la matriz de correlación de Pearson de las 14 variables. Con base a dicha matriz identifica ***los pares de variables independientes*** que tengan un coeficiente de correlación mayor o igual a 0.7 en valor absoluto. Obtener un tercer modelo de RLM y compara la significacia de los coeficientes y su coeficiente de determinación ajustado con respecto a los dos primeros modelos.
"""

# Dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# Correlación de Pearson
dataset.corr(method='pearson')

"""#### **Matriz de Correlación Pearson**

Una forma de ver la relación de las variable usando matplot y mapas de calor, lo que nos permite rápidamente identificar aquellas variables que tienen mayor relación entre sí. De igual manera podemos observar cláramente la diagonal, donde la relacón es 1 dado que comparamos la relación de cada variable consigo misma.
"""

import matplotlib.pyplot as plt

# Gráfica sencilla
#plt.matshow(dataset.corr(method='pearson'))
#plt.show()

df = dataset
f = plt.figure(figsize=(16, 12))
plt.matshow(df.corr(method='pearson'), fignum=f.number)
plt.xticks(range(df.shape[1]), df.columns, fontsize=12, rotation=45)
plt.yticks(range(df.shape[1]), df.columns, fontsize=12)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
plt.title('Matríz de Correlación', fontsize=16);

"""#### **Variables independientes con un coeficiente de correlación mayor o igual a 0.7 en valor absoluto.**"""

# Obtenemos valor absoluto de las variables
correl_matrix = dataset.corr(method='pearson').abs()
correl_pairs = correl_matrix.unstack()
# Eliminamos los pares de variables que tienen relación consigo mismas (diagonal)
independent_pairs = correl_pairs[correl_pairs < 1]
# Identificamos aquellos pares de variable cuya correlación es mayor o igual a 0.7
independent_pairs = independent_pairs[correl_pairs >= 0.7]
print(independent_pairs)

"""Las variables significativas para nuestro modelo son:
* INDUS
* NOX
* AGE
* DIS
* RAD
* TAX
* LSTAT

#### **Tercer modelo de RLM**
"""

# Repetimos los pasos de nuestro primer modelo
# Definimos el DF para el dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# De nuestras variable independeintes removemos las variable cuya correlación fue menor 0.7
x = dataset[["INDUS","NOX","AGE","DIS","RAD","TAX","LSTAT"]]
# Definimos variable dependiente
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
# Entrenamos nuestro modelo
mymodel = sm.OLS(y, xones).fit()
# Imprimimos los resultados
print(mymodel.summary())

"""#### **Comprar significancia de los coeficientes y su coeficiente de determinación ajustado con respecto a los primeros modelos.**

**Modelo 1**
* Coeficiente 𝑅² = 0.741
* Coeficiente 𝑅² Ajustado = 0.734

**Modelo 2**
* Coeficiente 𝑅² = 0.741
* Coeficiente 𝑅² Ajustado = 0.735

**Modelo 3**
* Coeficiente 𝑅² = 0.599
* Coeficiente 𝑅² Ajustado = 0.593

Comparamos los resultados vemos que nuestro tercer modelo tiene un menor desempeño que los dos modelos anteriores, esto tiene sentido ya que ahora sólo usamos 7 variables para explicar nuestro modelo, a pesar de remover la mitad de las variables el coficiente de determinación no disminuyó tanto ya que estas variable son las más significativas para el modelo.

### v. Con base a la matriz de correlación con las 14 variables, identifica ahora aquellas variables independientes que tengan una correlación de pearson mayor o igual a 0.5 con respecto a la variable dependiente MEDV. Obtener un cuarto modelo RLM y compararlo con los tres anteriores.

#### **Variables independientes con un coeficiente de correlación mayor o igual a 0.5 en valor absoluto.**
"""

# Obtenemos valor absoluto de las variables
correl_matrix = dataset.corr(method='pearson').abs()
correl_pairs = correl_matrix.unstack()
# Eliminamos los pares de variables que tienen relación consigo mismas (diagonal)
independent_pairs = correl_pairs[correl_pairs < 1]
# Identificamos aquellos pares de variable cuya correlación es mayor o igual a 0.5
independent_pairs = independent_pairs[correl_pairs >= 0.5]
print(independent_pairs)

"""Las variables que tienen relacióón con respecto a la variable dependiente MEDV son:
* RM
* PTRATIO
* LSTAT

Esto se obserba al final del listado anterior en la consola.

```
...
MEDV     RM         0.695360
         PTRATIO    0.507787
         LSTAT      0.737663
dtype: float64
```

#### **Cuarto modelo de RLM**
"""

# Repetimos los pasos de nuestro primer modelo
# Definimos el DF para el dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# De nuestras variable independeintes removemos las variable cuya correlación fue menor 0.5
x = dataset[["RM","PTRATIO","LSTAT"]]
# Definimos variable dependiente
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
# Entrenamos nuestro modelo
mymodel = sm.OLS(y, xones).fit()
# Imprimimos los resultados
print(mymodel.summary())

"""#### **Comprar significancia de los coeficientes y su coeficiente de determinación ajustado con respecto a los primeros modelos.**

**Modelo 1**
* Coeficiente 𝑅² = 0.741
* Coeficiente 𝑅² Ajustado = 0.734

**Modelo 2**
* Coeficiente 𝑅² = 0.741
* Coeficiente 𝑅² Ajustado = 0.735

**Modelo 3**
* Coeficiente 𝑅² = 0.599
* Coeficiente 𝑅² Ajustado = 0.593

**Modelo 4**
* Coeficiente 𝑅² = 0.679
* Coeficiente 𝑅² Ajustado = 0.677

Comparamos los resultados vemos que nuestro cuarto modelo tiene un mejor desempeño que el modelo anteriror y muy cercano a los primeros modelos, yesto usando 3 variables para explicar nuestro modelo, esto se debe a que sólo usamos aqueyas variables con una relacióón mayor o igual a 0.5 con respecto a la varaible MEDV, y son las las que mejor explican el modelo.

### vi. Usando únicamente la matriz 𝑋 con las 3 variables independientes del inciso anterior y la variable de salida MDEV, obtener la matriz de Pearson 𝑋⁺ = (𝑋ᵀ 𝑋)⁻¹𝑋ᵀ. Posteriormente obtener los coeficientes 𝛽 del modelo RLM resultante mediante la expresión: 𝛽 = 𝑋⁺𝑌. Compáralo con el inciso anterior y escribe tus conclusiones.
"""

dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# Variables identificadas en el inciso anterior
x = dataset[["RM","PTRATIO","LSTAT"]]
# Variable de salida MDEV
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)

"""#### **Matríz pseudo inversa de Moore-Penrose**"""

# Matriz pseudo inversa de Moore-Penrose
mat_penrose = np.linalg.pinv(xones)
print(mat_penrose)

"""#### **Coeficientes 𝛽 del modelo RLM**"""

from numpy.linalg import pinv
# calculamos coeficientes
b = pinv(xones).dot(y)
print(x.columns)
print(b)

"""#### **Comparación de los coeficientes del Modelo RLM**

**1. Coeficientes obtenidos con el modelo 4**

```
=======================
variable        coef  
-----------------------
const         18.5671
RM             4.5154 
PTRATIO       -0.9307
LSTAT         -0.5718
```





**2. Coeficientes Obtenidos con la matriz de Penrose**

```
===========================
variable        coef  
---------------------------
const         18.56711151
RM             4.51542094
PTRATIO       -0.93072256
LSTAT         -0.57180569
```


**En conclusion:**

Al analizar los coeficientes podemos observar que los que obtuvimos empleando la matriz de pearson tienen mayor presición que los que conseguimos en el modelo 4, pero si consideramos una presición de 4 decimales, los coeficientes en escencia resultan ser los mismos. Esto indica la utilidad de la matriz de pearson.

### vii. Finalmente escribe tus conclusiones de los resultados obtenidos.

#### **Conclusiones**

Gracias a los ejercicios realizados en esta tarea podemos concluir que es necesario un análisis profundo de los datos para poder seleccionar las variables independientes que resulten más significativas para representar nuestro modelo.

En los primero ejercicios descartamos variables como AGE e INDUS basándonos méramente en el *p-value* sin embargo en el tercer modelo estas variables son parte del set que sí utlizamos dado que su coeficiente de correlación era alto.

Otro punto importante fue el uso de la correlación con respecto a la variable independiente aquí fue claroque con solo 3 variables podíamos obtener resultados muyr cercanos a los observados en nuestro primer modelo de 14 variables, esto debido a que seleccionamos las variables que mayor relación tenían con la variable dependiente.

Finalmente podemos comprobar la utilidad de la Matrix pseudo inversa de Moore-Penrose para el análisis de coeficientes.
Con estas herramientas podremos decidir de una mejor manera que variables emplear para la generacion de modelos significativos y eficientes.
"""