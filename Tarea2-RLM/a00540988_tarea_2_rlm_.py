# -*- coding: utf-8 -*-
"""A00540988 Tarea 2-RLM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19BtrAnNxFxdonYgOqGt50zL59ieTVbcl

# **ITESM Campus Guadalajara**
## Aprendizaje de MÃ¡quinas
## Tarea 2 RegresiÃ³n Lineal MÃºltiple

###Â **A00540988 Luis Francisco GonzÃ¡lez RodÃ­guez**


---

# 1.  En este ejercicio usaremos la base de datos de Boston_housing_dataset (https://www.kaggle.com/apratim87/housingdata) la cual consta de 13 variables independientes y la variable dependiente. Son 506 datos de casas cuyo objetivo es la predicciÃ³n de su costo (MEDV).
"""

# Carga de librerias necesarias para leer y procesar los datos.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

#Cargar Data Set tal como se decargÃ³Ã³ del sitio de Kaggle
!curl -O https://raw.githubusercontent.com/Fragonz/MCC/master/Tarea2-RLM/data/datasets_270_583_housingdata.csv
!ls
!pwd
dspath = "/content/datasets_270_583_housingdata.csv"

"""**Examinemos el Data Set para ver su contenido.**"""

mydata = pd.read_csv(dspath, header=None)
mydata.head()

"""---
DespuÃ©s de examinar el dataset podemos comprobar que este archivo no contiene **headers** y debemos agregarlos siguiendo las instrucciones del sitio de Keggle: https://www.kaggle.com/apratim87/housingdata?select=housingdata.csv el cual nos dice que las columnas estan en orden y con las siguientes etiquetas:

**Variables in order:**

* CRIM - per capita crime rate by town
* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
* CHAS - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
* NOX - nitric oxides concentration (parts per 10 million)
* RM - average number of rooms per dwelling
* AGE - proportion of owner-occupied units built prior to 1940
* DIS - weighted distances to five Boston employment centres
* RAD - index of accessibility to radial highways
* TAX - full value property tax rate per 10'000
* PTRATIO - pupil teacher ratio by town
* B 1000 - (Bk - 0.63)^2 where Bk is the proportion of blacks by town
* LSTAT % - lower status of the population
* MEDV - Median value of owner-occupied homes in $1000's

---
"""

# Agregamos las etiquetas a las columnas 
column_labels=["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]
mydata.columns = column_labels

"""**Revisamos nuestro dataset con los nombres de columnas para poder manipular nuestros datos:**

---
"""

mydata.head()

# Examinemos el tamaÃ±o del DataSet para corroborar que tenemos 506 Datos
# AdemÃ¡s contamos con 14 columnas:
# 13 que corresponden a variables independientes y 1 variable dependiente.

mydata.shape

"""## a.  Obtener el modelo de RegresiÃ³n Lineal MÃºltiple (RLM) del modelo y con base a dicho modelo contestar los siguientes incisos:"""

# Dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# Variables Independientes
x = dataset[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT"]]
# Variable Dependiente
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
print(xones)

"""###Â **Primer Modelo de RegresiÃ³n Lineal MÃºltile (RLM)**"""

mymodel = sm.OLS(y, xones).fit()
print(mymodel.summary())

"""###Â i.   Interpretar el valor del coeficiente de determinaciÃ³n ajustado ğ‘…Â²

El coeficiente de determinaciÃ³n ajustado para nuestro modelo fue de 0.734, recordemos que este mide el porcentaje de variaciÃ³n de la variable dependiente teniendo en cuenta las variable independientes, es decir la variaciÃ³n es del 73.4%

### ii. Determina quÃ© variables no resultaron significativas considerendo un ğ‘£ğ‘ğ‘™ğ‘œğ‘Ÿ-ğ‘=0.01

Por observaciÃ³n en la tabla de nuestro modelo podemos concluir que las variables que no resultaron significativas, o con un *p-value* superior a 0.01 son **INDUS** con un *p-value* de 0.738 y **AGE** con un *p-value* de 0.958

### iii. Obtener un segundo modelo de RLM omitiendo las variables que no resultaron significativas en el inciso anterior. Comparar su coeficiente de determinaciÃ³n ajustado con base con respecto al primer modelo.

Para obtener un segundo modelo RLM, repetiremos los pasos del primer punto, pero esta vez removeremos las variables **INDUS** y **AGE**, las cuales resultaron NO significativas para nuestro modelo.
"""

# Repetimos los pasos de nuestro modelo anterior
# Definimos el DF para el dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# De nuestras variable independeintes removemos las variable INDUS y AGE que resultaron NO significativas para nuestro modelo
x = dataset[["CRIM","ZN","CHAS","NOX","RM","DIS","RAD","TAX","PTRATIO","BK","LSTAT"]]
# Definimos variable dependiente
y = dataset[["MEDV"]]

"""#### **Segundo Modelo RLM**"""

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
# Entrenamos nuestro modelo
mymodel = sm.OLS(y, xones).fit()
# Imprimimos los resultados
print(mymodel.summary())

"""#### **Comparar coeficiente de determinaciÃ³n ajustado con respecto al primer modelo**

* Coeficiente ğ‘…Â² Ajustado primer modelo = 73.4%
* Coeficiente ğ‘…Â² Ajustado segundo modelo = 73.5%

Esto significa que tuvimos una mejora de 0.001 al elininar dos variables, esto parece algo pequeÃ±o pero ahora podemos trabajar con un modelo de 11 variable y no de 13 como originalmente se habÃ­a planteado.

### iv. Obtener la matriz de correlaciÃ³n de Pearson de las 14 variables. Con base a dicha matriz identifica ***los pares de variables independientes*** que tengan un coeficiente de correlaciÃ³n mayor o igual a 0.7 en valor absoluto. Obtener un tercer modelo de RLM y compara la significacia de los coeficientes y su coeficiente de determinaciÃ³n ajustado con respecto a los dos primeros modelos.
"""

# Dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# CorrelaciÃ³n de Pearson
dataset.corr(method='pearson')

"""####Â **Matriz de CorrelaciÃ³n Pearson**

Una forma de ver la relaciÃ³n de las variable usando matplot y mapas de calor, lo que nos permite rÃ¡pidamente identificar aquellas variables que tienen mayor relaciÃ³n entre sÃ­. De igual manera podemos observar clÃ¡ramente la diagonal, donde la relacÃ³n es 1 dado que comparamos la relaciÃ³n de cada variable consigo misma.
"""

import matplotlib.pyplot as plt

# GrÃ¡fica sencilla
#plt.matshow(dataset.corr(method='pearson'))
#plt.show()

df = dataset
f = plt.figure(figsize=(16, 12))
plt.matshow(df.corr(method='pearson'), fignum=f.number)
plt.xticks(range(df.shape[1]), df.columns, fontsize=12, rotation=45)
plt.yticks(range(df.shape[1]), df.columns, fontsize=12)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
plt.title('MatrÃ­z de CorrelaciÃ³n', fontsize=16);

"""#### **Variables independientes con un coeficiente de correlaciÃ³n mayor o igual a 0.7 en valor absoluto.**"""

# Obtenemos valor absoluto de las variables
correl_matrix = dataset.corr(method='pearson').abs()
correl_pairs = correl_matrix.unstack()
# Eliminamos los pares de variables que tienen relaciÃ³n consigo mismas (diagonal)
independent_pairs = correl_pairs[correl_pairs < 1]
# Identificamos aquellos pares de variable cuya correlaciÃ³n es mayor o igual a 0.7
independent_pairs = independent_pairs[correl_pairs >= 0.7]
print(independent_pairs)

"""Las variables significativas para nuestro modelo son:
* INDUS
* NOX
* AGE
* DIS
* RAD
* TAX
* LSTAT

#### **Tercer modelo de RLM**
"""

# Repetimos los pasos de nuestro primer modelo
# Definimos el DF para el dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# De nuestras variable independeintes removemos las variable cuya correlaciÃ³n fue menor 0.7
x = dataset[["INDUS","NOX","AGE","DIS","RAD","TAX","LSTAT"]]
# Definimos variable dependiente
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
# Entrenamos nuestro modelo
mymodel = sm.OLS(y, xones).fit()
# Imprimimos los resultados
print(mymodel.summary())

"""####Â **Comprar significancia de los coeficientes y su coeficiente de determinaciÃ³n ajustado con respecto a los primeros modelos.**

**Modelo 1**
* Coeficiente ğ‘…Â² = 0.741
* Coeficiente ğ‘…Â² Ajustado = 0.734

**Modelo 2**
* Coeficiente ğ‘…Â² = 0.741
* Coeficiente ğ‘…Â² Ajustado = 0.735

**Modelo 3**
* Coeficiente ğ‘…Â² = 0.599
* Coeficiente ğ‘…Â² Ajustado = 0.593

Comparamos los resultados vemos que nuestro tercer modelo tiene un menor desempeÃ±o que los dos modelos anteriores, esto tiene sentido ya que ahora sÃ³lo usamos 7 variables para explicar nuestro modelo, a pesar de remover la mitad de las variables el coficiente de determinaciÃ³n no disminuyÃ³ tanto ya que estas variable son las mÃ¡s significativas para el modelo.

### v. Con base a la matriz de correlaciÃ³n con las 14 variables, identifica ahora aquellas variables independientes que tengan una correlaciÃ³n de pearson mayor o igual a 0.5 con respecto a la variable dependiente MEDV. Obtener un cuarto modelo RLM y compararlo con los tres anteriores.

#### **Variables independientes con un coeficiente de correlaciÃ³n mayor o igual a 0.5 en valor absoluto.**
"""

# Obtenemos valor absoluto de las variables
correl_matrix = dataset.corr(method='pearson').abs()
correl_pairs = correl_matrix.unstack()
# Eliminamos los pares de variables que tienen relaciÃ³n consigo mismas (diagonal)
independent_pairs = correl_pairs[correl_pairs < 1]
# Identificamos aquellos pares de variable cuya correlaciÃ³n es mayor o igual a 0.5
independent_pairs = independent_pairs[correl_pairs >= 0.5]
print(independent_pairs)

"""Las variables que tienen relaciÃ³Ã³n con respecto a la variable dependiente MEDV son:
* RM
* PTRATIO
* LSTAT

Esto se obserba al final del listado anterior en la consola.

```
...
MEDV     RM         0.695360
         PTRATIO    0.507787
         LSTAT      0.737663
dtype: float64
```

#### **Cuarto modelo de RLM**
"""

# Repetimos los pasos de nuestro primer modelo
# Definimos el DF para el dataset con el que vamos a trabajar
dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# De nuestras variable independeintes removemos las variable cuya correlaciÃ³n fue menor 0.5
x = dataset[["RM","PTRATIO","LSTAT"]]
# Definimos variable dependiente
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)
# Entrenamos nuestro modelo
mymodel = sm.OLS(y, xones).fit()
# Imprimimos los resultados
print(mymodel.summary())

"""####Â **Comprar significancia de los coeficientes y su coeficiente de determinaciÃ³n ajustado con respecto a los primeros modelos.**

**Modelo 1**
* Coeficiente ğ‘…Â² = 0.741
* Coeficiente ğ‘…Â² Ajustado = 0.734

**Modelo 2**
* Coeficiente ğ‘…Â² = 0.741
* Coeficiente ğ‘…Â² Ajustado = 0.735

**Modelo 3**
* Coeficiente ğ‘…Â² = 0.599
* Coeficiente ğ‘…Â² Ajustado = 0.593

**Modelo 4**
* Coeficiente ğ‘…Â² = 0.679
* Coeficiente ğ‘…Â² Ajustado = 0.677

Comparamos los resultados vemos que nuestro cuarto modelo tiene un mejor desempeÃ±o que el modelo anteriror y muy cercano a los primeros modelos, yesto usando 3 variables para explicar nuestro modelo, esto se debe a que sÃ³lo usamos aqueyas variables con una relaciÃ³Ã³n mayor o igual a 0.5 con respecto a la varaible MEDV, y son las las que mejor explican el modelo.

###Â vi. Usando Ãºnicamente la matriz ğ‘‹ con las 3 variables independientes del inciso anterior y la variable de salida MDEV, obtener la matriz de Pearson ğ‘‹âº = (ğ‘‹áµ€ ğ‘‹)â»Â¹ğ‘‹áµ€. Posteriormente obtener los coeficientes ğ›½ del modelo RLM resultante mediante la expresiÃ³n: ğ›½ = ğ‘‹âºğ‘Œ. CompÃ¡ralo con el inciso anterior y escribe tus conclusiones.
"""

dataset = pd.DataFrame(mydata[["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","BK","LSTAT","MEDV"]])
# Variables identificadas en el inciso anterior
x = dataset[["RM","PTRATIO","LSTAT"]]
# Variable de salida MDEV
y = dataset[["MEDV"]]

# Agregamos 1's a la primer columna como cosntante
xones = sm.add_constant(x)

"""#### **MatrÃ­z pseudo inversa de Moore-Penrose**"""

# Matriz pseudo inversa de Moore-Penrose
mat_penrose = np.linalg.pinv(xones)
print(mat_penrose)

"""#### **Coeficientes ğ›½ del modelo RLM**"""

from numpy.linalg import pinv
# calculamos coeficientes
b = pinv(xones).dot(y)
print(x.columns)
print(b)

"""#### **ComparaciÃ³n de los coeficientes del Modelo RLM**

**1. Coeficientes obtenidos con el modelo 4**

```
=======================
variable        coef  
-----------------------
const         18.5671
RM             4.5154 
PTRATIO       -0.9307
LSTAT         -0.5718
```





**2. Coeficientes Obtenidos con la matriz de Penrose**

```
===========================
variable        coef  
---------------------------
const         18.56711151
RM             4.51542094
PTRATIO       -0.93072256
LSTAT         -0.57180569
```


**En conclusion:**

Al analizar los coeficientes podemos observar que los que obtuvimos empleando la matriz de pearson tienen mayor presiciÃ³n que los que conseguimos en el modelo 4, pero si consideramos una presiciÃ³n de 4 decimales, los coeficientes en escencia resultan ser los mismos. Esto indica la utilidad de la matriz de pearson.

### vii. Finalmente escribe tus conclusiones de los resultados obtenidos.

#### **Conclusiones**

Gracias a los ejercicios realizados en esta tarea podemos concluir que es necesario un anÃ¡lisis profundo de los datos para poder seleccionar las variables independientes que resulten mÃ¡s significativas para representar nuestro modelo.

En los primero ejercicios descartamos variables como AGE e INDUS basÃ¡ndonos mÃ©ramente en el *p-value* sin embargo en el tercer modelo estas variables son parte del set que sÃ­ utlizamos dado que su coeficiente de correlaciÃ³n era alto.

Otro punto importante fue el uso de la correlaciÃ³n con respecto a la variable independiente aquÃ­ fue claroque con solo 3 variables podÃ­amos obtener resultados muyr cercanos a los observados en nuestro primer modelo de 14 variables, esto debido a que seleccionamos las variables que mayor relaciÃ³n tenÃ­an con la variable dependiente.

Finalmente podemos comprobar la utilidad de la Matrix pseudo inversa de Moore-Penrose para el anÃ¡lisis de coeficientes.
Con estas herramientas podremos decidir de una mejor manera que variables emplear para la generacion de modelos significativos y eficientes.
"""